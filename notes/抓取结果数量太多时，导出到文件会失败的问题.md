issues：[Exporting Crawl Results not working](https://github.com/xuejianxianzun/PixivBatchDownloader/issues/131)

抓取结果数量太多时，导出抓取结果会失败的问题

测试：https://www.pixiv.net/users/973675/following

抓取这个用户关注的所有用户的作品，结果总数约为 353,098。

抓取完成后，该页面内存占用 542 MB。

导出 CSV 文件：可以成功导出 CSV 文件，导出时该页面内存占用升到 1035 MB（+513），导出完成后回落。（CSV 文件体积 89.2 MB）

导出抓取结果（为 json 文件）：导出失败，该页面内存占用升到 1768 MB（+1256），然后报错，然后内存回落。（系统内存容量充足，并非是内存用尽）

报错信息：

```
Uncaught RangeError: Invalid string length
at JSON.stringify (<anonymous>)
at Function.json2Blob
```

根据搜索，原因是 V8 引擎对于单个字符串长度有限制，在 64 位操作系统上，这个限制导致了单个字符串变量的体积不能超过 1 GB。当抓取结果太多时，使用 `JSON.stringify` 把所有结果转换成一个字符串，这个字符串的长度超过了上限，导致浏览器报错。

资料来源：https://chromium-review.googlesource.com/c/v8/v8/+/2030916

----------

下面是解决这个问题的过程。

**避免产生这个错误**

以前会导致问题的代码：

```js
// 把所有结果转换成字符串
const str = JSON.stringify(store.result, null, 2)
// 创建 blob 对象
const blob = new Blob([str], { type: 'application/json' })
```

现在，我不再把所有结果一次性格式化为字符串，而是建立一个数组，存放每一条结果对应的字符串。最后把这个数组转换成 BLOB 对象。这样就不会产生一个超长的字符串，避免了这个错误。

修改后的代码如下：

```js
// 使用数组储存文件数据
let resultArray:string[] = []
// 定义数组项的分隔字符
const split = ','
// 在数组开头添加数组的开始符号
resultArray.push('[')
// 循环添加每一个结果，以及分割字符
for (const result of store.result) {
  resultArray.push(JSON.stringify(result, null, 2))
  resultArray.push(split)
}
// 删除最后一个分隔符（不去掉的话会导致格式错误）
resultArray.pop()
// 在数组末尾添加数组的结束符号
resultArray.push(']')
// 创建 blob 对象
const blob = new Blob(resultArray, { type: 'application/json' })
resultArray = []
```

这样成功的导出了全部结果（353,098 个），文件体积 547 MB，页面内存峰值 3916 MB（+3374）。

**进一步减小体积**

之前的代码里，把数据转换成字符串时设置了格式：

```js
JSON.stringify(result, null, 2)
```

这些格式会导致产生的结果字符串更长（内存占用也会增加），现在去掉格式：

```js
JSON.stringify(result)
```

这样不会因为格式而产生多余的换行、空格。此时导出全部结果，文件体积 450 MB，页面内存峰值 3269 MB（+2727）。

与之前相比，文件体积减少了约 18%，内存使用量减少了大约 19%。

**导出为多个文件（未实现）**

虽然上面解决了问题，并且进行了优化，但是内存的使用量仍然很高。如果结果数量更多的话（假设有 600,000 个结果），导出结果为 json 文件时，页面可能会崩溃。（当然，这也和用户的硬件、使用环境有关）

解决这个问题的一个思路是导出为多个文件，例如每 100,000 个结果为一批，导出到一个文件里。

但是这样的话，导入时就需要依次导入多个文件，这样操作很不方便。并且下载器也需要对此进行一定的修改。

考虑到抓取结果如此多的情况是极小概率发生的，所以目前没有实现分批导出的功能。

如果有人说，我这次抓取就是会产生几十万上百万个结果，无法导出怎么办呢？有个实用的办法，就是分批抓取。通过设置页数范围，一次只抓取一部分，这样无论是下载还是导出都不会产生问题。

----------

导入的效率：

打开一个新的页面，页面内存使用量 153 MB。然后导入之前导出的文件（35,3098 个结果），等待了大约一分钟时间，内存使用量峰值约 1770 MB（+1617）。

导入所花费的时间是比较多的。因为导入时下载器不是直接把所有结果直接存储进 store，而是对于每一条结果都重新执行了一遍添加流程。这是为了能够应用过滤条件，所以慢一些也值得。

这些数据的导出速度比较快，断点续传的恢复速度也比较快。

----------

导出 CSV 文件时，如果结果数量更多，会因为字符串长度达到限制而发生错误吗？

目前看来有这个可能，因为在把 CSV 文件的数据转换成 BLOB 时，把所有数据合并成了字符串：

```ts
// 把结果数组以换行符连接，转换成一个字符串
result.join(this.CRLF)
```

如果抓取结果太多，这也可能会产生错误。虽然目前还没有遇到过这种问题，我还是修改了代码以避免产生错误。

现在数据会保持在数组里，不会转换成一个字符串了。

```ts
// 添加每一行的数据和换行符
for (const row of data) {
  result.push(this.format(row).join(this.separate))
  result.push(this.CRLF)
}
```